#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# hypermodel.py
# Copyright (C) 2022 flossCoder
# 
# hypermodel is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# hypermodel is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program.  If not, see <http://www.gnu.org/licenses/>.
"""
Created on Wed Mar 23 10:30:01 2022

@author: flossCoder
"""

import numpy as np
import os
import tensorflow as tf
import keras_tuner as kt
from contextlib import redirect_stdout
import pickle
from copy import deepcopy

from data_generator import ClassificationSequence, TrainingValidationSetCuiGenerator
import auxiliary_ml_functions as aux
from model_tester import evaluate_model, aggregate_prediction_results
import class_balanced_loss as cb_loss

class AbstractHyperModel(kt.HyperModel):
    """
    This class defines an abstract hyperparameter model to save some parameter.
    """
    def __init__(self, image_resolution, number_of_classes, loss = "categorical_crossentropy", metrics = ['accuracy'], cnn_trainable = False):
        """
        This function initializes the AbstractHyperModel.

        Parameters
        ----------
        image_resolution : tuple
            The input resolution of the images.
        number_of_classes : int
            The number of classes of the dataset.
        loss : string or function handle, optional
            The loss is eather a string of a build in loss of keras or a function handle. The default is categorical_crossentropy.
        metrics : list of strings or function handles, optional
            The metrics used for evaluation is a list containing eather strings of build in keras functions or function handles.The default is ['accuracy'].
        cnn_trainable : boolean, optional
            This flag indicates, whether the cnn weights shall be fine tuned (True) or not (False). The default is False.

        Returns
        -------
        None.

        """
        self.image_resolution = image_resolution + (3,)
        self.number_of_classes = number_of_classes
        self.loss = loss
        self.metrics = metrics
        self.cnn_trainable = cnn_trainable
        self.search_hyperparameters = False
    
    def get_loss(self):
        """
        This function returns the loss

        Returns
        -------
        loss : string or function handle
            The loss is eather a string of a build in loss of keras or a function handle.

        """
        return self.loss
    
    def set_search_hyperparameters(self, search_hyperparameters):
        """
        This function sets the search hyperparameters flag.

        Parameters
        ----------
        search_hyperparameters : boolean
            True: the object is used for hyperparameter search,
            False: the object is not used for hyperparameter search.

        Returns
        -------
        None.

        """
        self.search_hyperparameters = search_hyperparameters
    
    def build(self, hps):
        """
        This function defines the model builder, which must be implemented
        with the signature of the abstract function by the inheriting function.

        Parameters
        ----------
        hps : HyperParameters
            The hyperparameters object for the model.

        Raises
        ------
        NotImplementedError
            The NotImplementedError is raised in case this function is not overriden.

        Returns
        -------
        model : keras model
            The keras / tensorflow model that has to be generated by the function.

        """
        raise NotImplementedError("The build function has not been implemented yet!")

class AbstractRebalanceCuiHyperModel(AbstractHyperModel):
    """
    This class defines an abstract hyperparameter model to save some parameter
    and overwrites the fit function to incorporate a two phase training scheme.
    """
    def __init__(self, image_resolution, number_of_classes, loss = "categorical_crossentropy", metrics = ['accuracy'], cnn_trainable = False, main_generator = None, cbl_wrapper = None):
        """
        This function initializes the AbstractRebalanceCuiHyperModel.

        Parameters
        ----------
        image_resolution : tuple
            The input resolution of the images.
        number_of_classes : int
            The number of classes of the dataset.
        loss : string or function handle, optional
            The loss is eather a string of a build in loss of keras or a function handle. The default is categorical_crossentropy.
        metrics : list of strings or function handles, optional
            The metrics used for evaluation is a list containing eather strings of build in keras functions or function handles.The default is ['accuracy'].
        cnn_trainable : boolean, optional
            This flag indicates, whether the cnn weights shall be fine tuned (True) or not (False). The default is False.
        main_generator : TrainingValidationSetCuiGenerator, optional
            The main_generator is the object, which is responsible for the generation of training- and validation generator. The default is None.
        cbl_wrapper : function handle, optional
            The wrapper function of the class balanced categorical crossentropy loss. The default is None.

        Returns
        -------
        None.

        """
        AbstractHyperModel.__init__(self, image_resolution, number_of_classes, loss, metrics, cnn_trainable)
        if type(main_generator) != type(None):
            self.main_generator = main_generator
        else:
            self.main_generator = None
        if type(cbl_wrapper) != type(None):
            self.cbl_wrapper = cbl_wrapper
        else:
            self.cbl_wrapper = None
    
    def fit(self, hps, model, *args, **kwargs):
        """
        This function provides a two phase training scheme.

        Parameters
        ----------
        hps : HyperParameters
            The hyperparameters for the model.
        model :  keras model
            The keras / tensorflow model.
        *args : tuple
            The arguments.
        **kwargs : dict
            The keyword arguments.

        Returns
        -------
        keras callbacks history
            The history object of the training phase.

        """
        # check, if learning_rate_factor or stage_split are tuned by the hps object
        hps_names = [i.name for i in hps.space]
        if "learning_rate_factor" in hps_names:
            kwargs["learning_rate_factor"] = hps.get("learning_rate_factor")
        if "stage_split" in hps_names:
            kwargs["stage_split"] = hps.get("stage_split")
        
        kwargs_first_phase, kwargs_second_phase, learning_rate_factor = aux.prepare_kwargs_rebalance_fit(**kwargs)
        
        history_first_phase = super().fit(hps, model, *args, **kwargs_first_phase)
        # adapt the learning rate
        try:
            # use normal optimizer
            tf.keras.backend.set_value(model.optimizer.learning_rate, model.optimizer.learning_rate / learning_rate_factor)
        except:
            # use multioptimizer
            for optimizer_spec in model.optimizer.optimizer_specs:
                tf.keras.backend.set_value(optimizer_spec['optimizer'].learning_rate, optimizer_spec['optimizer'].learning_rate / learning_rate_factor)
        
        if type(self.main_generator) != type(None):
            # rebalance the main generator
            self.main_generator.equilibrate_distribution()
        if type(self.cbl_wrapper) != type(None):
            # adapt the cbl pre factor state
            self.cbl_wrapper.pre_factor = cb_loss.class_balanced_pre_factor(np.concatenate((self.main_generator.training_generator.images_class_labels, self.main_generator.validation_generator.images_class_labels)), self.cbl_wrapper.beta)
        
        second_callback = None
        if "callbacks" in kwargs_second_phase.keys():
            for i in range(len(kwargs_second_phase["callbacks"])):
                if type(kwargs_second_phase["callbacks"][i]) == type(tf.keras.callbacks.ModelCheckpoint("")):
                    second_callback = "_.".join(kwargs_second_phase["callbacks"][i].filepath.split("."))
                    kwargs_second_phase["callbacks"][i].filepath = second_callback
        history_second_phase = super().fit(hps, model, *args, **kwargs_second_phase)
        
        if type(second_callback) != type(None) and not self.search_hyperparameters:
            aux.rename_files("", [second_callback.replace("{epoch:02d}", "%s")%(f"{epoch:02d}") for epoch in range(1, kwargs_second_phase["epochs"] + 1)], [".".join(second_callback.split("_.")).replace("{epoch:02d}", "%s")%(f"{epoch:02d}") for epoch in range(kwargs_first_phase["epochs"] + 1, kwargs_first_phase["epochs"] + kwargs_second_phase["epochs"] + 1)])
        
        return aux.join_history_objects(history_first_phase, history_second_phase)
    
    def set_main_generator(self, main_generator):
        """
        This function sets the main_generator object.

        Parameters
        ----------
        main_generator : TrainingValidationSetCuiGenerator
            The main_generator is the object, which is responsible for the generation of training- and validation generator.

        Returns
        -------
        None.

        """
        self.main_generator = main_generator
    
    def set_cbl_wrapper(self, cbl_wrapper):
        """
        This function sets the cbl_wrapper object.

        Parameters
        ----------
        cbl_wrapper : function handle
            The wrapper function of the class balanced categorical crossentropy loss.

        Returns
        -------
        None.

        """
        self.cbl_wrapper = cbl_wrapper

def initialize_tuner(model_builder, output_wd, output_name, seed, objective = "val_accuracy", keras_tuner_class = "BayesianOptimization", **kwargs):
    """
    This function sets up the keras tuner object.

    Parameters
    ----------
    model_builder : function handle or object
        The function / object defining the model.
    output_wd : string
        The working directory for the output.
    output_name : string
        The output name of the experiment.
    seed : int
        The seed for the random number generator.
    objective : string, optional
        The objective function for validation. The default is "val_accuracy".
    keras_tuner_class : string, optional
        The keras_tuner_class denotes the keras tuner used for hyperparameter optimization.
        Allowed values: "BayesianOptimization", "Hyperband" and "RandomSearch".
        The default is "BayesianOptimization".
    **kwargs : dict
        The keyword arguments for passing parameters to the optimizer. For details see the code below and the keras tuner doku.

    Raises
    ------
    Exception
        The exception is raised, in case there are missing kwargs parameter.

    Returns
    -------
    tuner : keras tuner object
        The keras tuner object.

    """
    if keras_tuner_class == "BayesianOptimization":
        
        bayes_kwargs = {}
        max_trials = None
        for key, value in kwargs.items():
            if key == "max_trials":
                max_trials = value
            elif key == "num_initial_points":
                bayes_kwargs[key] = value
            elif key == "alpha":
                bayes_kwargs[key] = value
            elif key == "beta":
                bayes_kwargs[key] = value
            elif key == "hyperparameters":
                bayes_kwargs[key] = value
            elif key == "tune_new_entries":
                bayes_kwargs[key] = value
            elif key == "allow_new_entries":
                bayes_kwargs[key] = value
        
        if type(max_trials) == type(None):
            raise Exception("max_trails in kwargs for BayesianOptimization missing")
        
        tuner = kt.BayesianOptimization(model_builder, objective, max_trials, seed = seed, directory = output_wd, project_name = output_name, **bayes_kwargs)
    elif keras_tuner_class == "Hyperband":
        
        hyperband_kwargs = {}
        max_epochs = None
        for key, value in kwargs.items():
            if key == "max_epochs":
                max_epochs = value
            elif key == "factor":
                hyperband_kwargs[key] = value
            elif key == "hyperband_iterations":
                hyperband_kwargs[key] = value
            elif key == "hyperparameters":
                hyperband_kwargs[key] = value
            elif key == "tune_new_entries":
                hyperband_kwargs[key] = value
            elif key == "allow_new_entries":
                hyperband_kwargs[key] = value
        
        if type(max_epochs) == type(None):
            raise Exception("max_epochs in kwargs for Hyperband missing")
        
        tuner = kt.Hyperband(model_builder, objective, max_epochs, seed = seed, directory = output_wd, project_name = output_name, **hyperband_kwargs)
    elif keras_tuner_class == "RandomSearch":
        
        random_kwargs = {}
        max_trials = None
        for key, value in kwargs.items():
            if key == "max_trials":
                max_trials = value
            elif key == "hyperparameters":
                random_kwargs[key] = value
            elif key == "tune_new_entries":
                random_kwargs[key] = value
            elif key == "allow_new_entries":
                random_kwargs[key] = value
        
        if type(max_trials) == type(None):
            raise Exception("max_trails in kwargs for RandomSearch missing")
        
        tuner = kt.RandomSearch(model_builder, objective, max_trials, seed = seed, directory = output_wd, project_name = output_name, **random_kwargs)
    
    return tuner

def build_hypermodel(model_builder, hps, output_wd, output_name, seed, objective = "val_accuracy", keras_tuner_class = "BayesianOptimization", **kwargs):
    """
    This function sets up the hypermodel.

    Parameters
    ----------
    model_builder : function handle or object
        The function / object defining the model.
    hps : HyperParameters
        The best hyperparameters for the model.
    output_wd : string
        The working directory for the output.
    output_name : string
        The output name of the experiment.
    seed : int
        The seed for the random number generator.
    objective : string, optional
        The objective function for validation. The default is "val_accuracy".
    keras_tuner_class : string, optional
        The keras_tuner_class denotes the keras tuner used for hyperparameter optimization.
        Allowed values: "BayesianOptimization", "Hyperband" and "RandomSearch".
        The default is "BayesianOptimization".
    **kwargs : dict
        The keyword arguments for passing parameters to the optimizer. For details see the code below and the keras tuner doku.

    Returns
    -------
    model :  keras model
        The keras / tensorflow model generated with the best hyperparameters.

    """
    tuner = initialize_tuner(model_builder, output_wd, output_name, seed, objective, keras_tuner_class, **kwargs)
    model = tuner.hypermodel.build(hps)
    aux.save_compiled_model(model, output_wd, output_name)
    return model

def setup_main_generator(temp_filename_dir, training_image_line_index, labels_training, validation_image_line_index, labels_validation, batch_size, num_epochs, keras_net_application, seed, generator_class, number_of_classes = None, resolution = None, shuffel_training_samples = False, **kwargs):
    """
    This function is responsible for setting up the data generator.

    Parameters
    ----------
    temp_filename_dir : string
        The path and filename to the temporary directory.
    training_image_line_index : numpy array
        The index of the temp file for the trainings set.
    labels_training : numpy array
        The labels of the trainings set.
    validation_image_line_index : numpy array
        The index of the temp file for the validation set.
    labels_validation : numpy array
        The labels of the validation set.
    batch_size : int
        The number of samples per training batch.
    num_epochs : int
        The number of training epochs.
    keras_net_application : module handle
        This module handle addresses a tf.keras.applications.MODULE module, such that keras_net_application.preprocess_input is valid.
    seed : integer
        The seed used for numpy random to guarantee reproducibility, if it is not None.
    generator_class : sting
        The generator_class denotes which generator shall be used. Valid are ClassificationSequence and TrainingValidationSetCuiGenerator.
    number_of_classes : int, optional
        The number of classes, if None, take the number of unique labels as number of classes. The default is None.
    resolution : tuple or float, optional
        The resolution describes the output size of an image. If resolution
        contains a tuple with two ints, they are used as resolution. If it
        contains a float, the number is considered as scaling factor preserving
        the original resolution. The default is None.
    shuffel_training_samples : boolean, optional
        Shuffel training samples denotes, whether the dataset shall be shuffeled (if True) after each epoch. The default is False.
    **kwargs : dict
        The keyword arguments for passing parameters to the optimizer or the datagenerator.
        For details see the code below, the keras tuner doku and the datagenerator doku.

    Raises
    ------
    Exception
        The Exception is raised in case an invalid generator class is given.

    Returns
    -------
    main_generator : TrainingValidationSetCuiGenerator
        The TrainingValidationSetCuiGenerator is the main_generator.
        In case a simple generator is used, this return remains None.
    train_generator : derived class from tf.keras.utils.Sequence
        The data generator for training.
    val_generator : derived class from tf.keras.utils.Sequence
        The data generator for validation.

    """
    if generator_class == "ClassificationSequence":
        main_generator = None
        train_kwargs = deepcopy(kwargs)
        if "train_augmentation_list" in kwargs.keys():
            del train_kwargs["train_augmentation_list"]
            train_kwargs["augmentation_list"] = kwargs["train_augmentation_list"]
        val_kwargs = deepcopy(kwargs)
        if "val_augmentation_list" in kwargs.keys():
            del val_kwargs["val_augmentation_list"]
            val_kwargs["augmentation_list"] = kwargs["val_augmentation_list"]
        train_generator = ClassificationSequence(temp_filename_dir, training_image_line_index, labels_training, batch_size, keras_net_application, number_of_classes, resolution, shuffel_training_samples, seed, **train_kwargs)
        val_generator = ClassificationSequence(temp_filename_dir, validation_image_line_index, labels_validation, batch_size, keras_net_application, number_of_classes, resolution, shuffel_training_samples, seed, **val_kwargs)
    elif generator_class == "TrainingValidationSetCuiGenerator":
        if "split_seed" in kwargs.keys():
            split_seed = kwargs["split_seed"]
            del kwargs["split_seed"]
        else:
            split_seed = None
        if "validation_split" in kwargs.keys():
            validation_split = kwargs["validation_split"]
            del kwargs["validation_split"]
        else:
            validation_split = 0.1
        main_generator = TrainingValidationSetCuiGenerator(temp_filename_dir, training_image_line_index, labels_training, batch_size, num_epochs, keras_net_application, number_of_classes, validation_split, resolution, shuffel_training_samples, seed, split_seed, **kwargs)
        main_generator.generate_train_val_split()
        train_generator, val_generator = main_generator.get_train_val_objects()
    else:
        raise Exception("Invalid generator_class " + generator_class)
    return main_generator, train_generator, val_generator

def train_model(model, output_wd, output_name, train_generator, val_generator, num_epochs, model_builder = None, hps = None):
    """
    This function performes the training.

    Parameters
    ----------
    model : keras model
        The keras / tensorflow model based on inception v3.
    output_wd : string
        The working directory for the output.
    output_name : string
        The output name of the experiment.
    train_generator : derived class from tf.keras.utils.Sequence
        The data generator for training.
    val_generator : derived class from tf.keras.utils.Sequence
        The data generator for validation.
    num_epochs : int
        The number of training epochs.
    model_builder : function handle or object
        The function / object defining the model.
    hps : HyperParameters
        The best hyperparameters for the model.

    Returns
    -------
    model :  keras model
        The keras / tensorflow model.
    history : keras history
        The history of the training.

    """
    
    callback = tf.keras.callbacks.ModelCheckpoint(filepath = os.path.join(output_wd, "%s_{epoch:02d}.h5"%output_name), save_weights_only = True, save_freq = "epoch")
    if hps is None or model_builder is None:
        history = model.fit(x = train_generator, validation_data = val_generator, epochs = num_epochs, callbacks = [callback])
    else:
        history = model_builder.fit(hps, model, x = train_generator, validation_data = val_generator, epochs = num_epochs, callbacks = [callback])
    np.save(os.path.join(output_wd, "%s_hist.npy"%output_name), history.history)
    return model, history

def search_hyperparameters(model_builder, output_wd, output_name, temp_filename_dir, training_image_line_index, labels_training, validation_image_line_index, labels_validation, batch_size, num_epochs, keras_net_application, seed, generator_class, objective = "val_accuracy", number_of_classes = None, resolution = None, shuffel_training_samples = False, keras_tuner_class = "BayesianOptimization", **kwargs):
    """
    This function performes the parameter search.

    Parameters
    ----------
    model_builder : function handle or object
        The function / object defining the model.
    output_wd : string
        The working directory for the output.
    output_name : string
        The output name of the experiment.
    temp_filename_dir : string
        The path and filename to the temporary directory.
    training_image_line_index : numpy array
        The index of the temp file for the trainings set.
    labels_training : numpy array
        The labels of the trainings set.
    validation_image_line_index : numpy array
        The index of the temp file for the validation set.
    labels_validation : numpy array
        The labels of the validation set.
    batch_size : int
        The number of samples per training batch.
    num_epochs : int
        The number of training epochs.
    keras_net_application : module handle
        This module handle addresses a tf.keras.applications.MODULE module, such that keras_net_application.preprocess_input is valid.
    seed : integer
        The seed used for numpy random to guarantee reproducibility, if it is not None.
    generator_class : sting
        The generator_class denotes which generator shall be used. Valid are ClassificationSequence and TrainingValidationSetCuiGenerator.
    objective : string, optional
        The objective function for validation. The default is "val_accuracy".
    number_of_classes : int, optional
        The number of classes, if None, take the number of unique labels as number of classes. The default is None.
    resolution : tuple or float, optional
        The resolution describes the output size of an image. If resolution
        contains n tuple with two ints, they are used as resolution. If it
        contains a float, the number is considered as scaling factor preserving
        the original resolution. The default is None.
    shuffel_training_samples : boolean, optional
        Shuffel training samples denotes, whether the dataset shall be shuffeled (if True) after each epoch. The default is False.
    keras_tuner_class : string, optional
        The keras_tuner_class denotes the keras tuner used for hyperparameter optimization.
        Allowed values: "BayesianOptimization", "Hyperband" and "RandomSearch".
        The default is "BayesianOptimization".
    **kwargs : dict
        The keyword arguments for passing parameters to the optimizer or the datagenerator.
        For details see the code below, the keras tuner doku and the datagenerator doku.
    
    Returns
    -------
    None.

    """
    model_builder.set_search_hyperparameters(True)
    main_generator, train_generator, val_generator = setup_main_generator(temp_filename_dir, training_image_line_index, labels_training, validation_image_line_index, labels_validation, batch_size, num_epochs, keras_net_application, seed, generator_class, number_of_classes, resolution, shuffel_training_samples, **kwargs)
    if type(main_generator) != type(None):
        model_builder.set_main_generator(main_generator)
    
    tuner = initialize_tuner(model_builder, output_wd, output_name, seed, objective, keras_tuner_class, **kwargs)
    
    with open(os.path.join(output_wd, "%s_tuner_summary.txt"%output_name), "w") as f:
        with redirect_stdout(f):
            tuner.search_space_summary()
    
    tuner.search(x = train_generator, validation_data = val_generator, epochs = num_epochs)
    hps = tuner.get_best_hyperparameters(num_trials = 1)[0]
    pickle.dump(hps, open(os.path.join(output_wd, "%s_hps.pkl"%output_name), "wb"))
    np.save(os.path.join(output_wd, "%s_hps.npy"%output_name), hps.values)

def main_function_train_test_hyperparameters(prediction, epoch, output_wd, output_name, temp_filename_dir, training_image_line_index, labels_training, validation_image_line_index, labels_validation, test_image_line_index, labels_test, batch_size, num_epochs, keras_net_application, generator_class, number_of_classes, shuffel_training_samples, seed, model_builder, hps_filename, loss, k, resolution = None, objective = "val_accuracy", keras_tuner_class = "BayesianOptimization", **kwargs):
    """
    This function is used to performe the magic required to validate the optimal hyperparameters.

    Parameters
    ----------
    prediction : boolean
        Boolean flag to switch between prediction (True) and training (False).
    epoch : int
        The epoch for loading the model. If None, the results are aggregated.
    output_wd : string
        The working directory for the output.
    output_name : string
        The output name of the experiment.
    temp_filename_dir : string
        The path and filename to the temporary directory.
    training_image_line_index : numpy array
        The index of the temp file for the trainings set.
    labels_training : numpy array
        The labels of the trainings set.
    validation_image_line_index : numpy array
        The index of the temp file for the validation set.
    labels_validation : numpy array
        The labels of the validation set.
    test_image_line_index : numpy array
        The index of the temp file for the test set.
    labels_test : numpy array
        The labels of the test set.
    batch_size : int
        The number of samples per training batch.
    num_epochs : int
        The number of epochs for aggregating results.
    keras_net_application : module handle
        This module handle addresses a tf.keras.applications.MODULE module, such that keras_net_application.preprocess_input is valid.
    generator_class : string
        The generator_class denotes which generator shall be used. Valid are ClassificationSequence and TrainingValidationSetCuiGenerator.
    number_of_classes : int
        The number of classes, if None, take the number of unique labels as number of classes.
    shuffel_training_samples : boolean
        Shuffel training samples denotes, whether the dataset shall be shuffeled (if True) after each epoch.
    seed : integer
        The seed used for numpy random to guarantee reproducibility, if it is not None.
    model_builder : function handle or object
        The function / object defining the model.
    hps_filename : string
        The filename of the best hyperparameters for the model.
    loss : string or function handle
        The loss is eather a string of a build in loss of keras or a function handle.
    k : int
        The k of top k accuracy.
    resolution : tuple or float, optional
        The resolution describes the output size of an image. If resolution
        contains a tuple with two ints, they are used as resolution. If it
        contains a float, the number is considered as scaling factor preserving
        the original resolution. The default is None.
    objective : string, optional
        The objective function for validation. The default is "val_accuracy".
    keras_tuner_class : string, optional
        The keras_tuner_class denotes the keras tuner used for hyperparameter optimization.
        Allowed values: "BayesianOptimization", "Hyperband" and "RandomSearch".
        The default is "BayesianOptimization".
    **kwargs : dict
        The keyword arguments for passing parameters to the optimizer. For details see the code below and the keras tuner doku.
    
    
    Returns
    -------
    None.

    """
    if prediction:
        if type(loss) == type(None):
            loss = model_builder.get_loss()
            if type(loss) == type(None):
                hps = pickle.load(open(os.path.join(output_wd, "%s.pkl"%hps_filename), "rb"))
                main_generator, train_generator, val_generator = setup_main_generator(temp_filename_dir, training_image_line_index, labels_training, validation_image_line_index, labels_validation, batch_size, num_epochs, keras_net_application, seed, generator_class, number_of_classes, resolution, shuffel_training_samples, **kwargs)
                if type(main_generator) != type(None):
                    model_builder.set_main_generator(main_generator)
                model = build_hypermodel(model_builder, hps, output_wd, output_name, seed, objective, keras_tuner_class, **kwargs)
                loss = model_builder.get_loss()
        if type(epoch) != type(None):
            evaluate_model(output_wd, output_name, temp_filename_dir, test_image_line_index, labels_test, loss, k, batch_size, keras_net_application, number_of_classes, resolution, False, seed, epoch)
        else:
            aggregate_prediction_results(output_wd, output_name, num_epochs)
    else:
        hps = pickle.load(open(os.path.join(output_wd, "%s.pkl"%hps_filename), "rb"))
        main_generator, train_generator, val_generator = setup_main_generator(temp_filename_dir, training_image_line_index, labels_training, validation_image_line_index, labels_validation, batch_size, num_epochs, keras_net_application, seed, generator_class, number_of_classes, resolution, shuffel_training_samples, **kwargs)
        if type(main_generator) != type(None):
            model_builder.set_main_generator(main_generator)
        model = build_hypermodel(model_builder, hps, output_wd, output_name, seed, objective, keras_tuner_class, **kwargs)
        if type(loss) == type(None):
            loss = model_builder.get_loss()
        [model, history] = train_model(model, output_wd, output_name, train_generator, val_generator, num_epochs, model_builder, hps)
